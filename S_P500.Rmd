
---
title: "Standard & Poor's 500 INDEX Forecasting (GSPC)"
always_allow_html: yes
output: github_document
---
##Abstract
Post the financial debacle in 2008-09, the necessity for forecasting the stock market has become of paramount importance. With advent of tools for forecasting and the ease of availability of data, there is no restriction on the different forecasting techniques.  In this paper we are attempting to find the optimal statistical technique to forecast the stock market based on the S&P 500 index.   Our choice of S&P 500 Index in the presence of other indices such as Dow Jones Industrial Average and Nasdaq Composite Index, is mainly as S&P 500 provides a broader representation of the overall US stock market.                     

#Introduction
Brief Note on S&P 500 Index
The Standard & Poor's 500, often abbreviated as the S&P 500, or just "the S&P", is an American stock market index based on the market capitalizations of 500 large companies having common stock listed on the NYSE or NASDAQ. The S&P 500 index components and their weightings are determined by S&P Dow Jones Indices.
Forecasting of the stock market can be carried out vide two approaches: -
●	Fundamental Approach –  forecasting is done based on the economy, the financial statement of the company and the company’s future prospects
●	Technical Analysis – forecasting based on time series analysis of available historical data of the stock market.

In this project, we have attempted only the second approach i.e “Technical Analysis”.  The data for our project is obtained from “Yahoo Finance” and the forecasting techniques are implemented in R. 

```{r warning=FALSE ,echo =FALSE, message= FALSE}
library(fpp) # forecasting 
library(astsa) # advanced stastical time series analysis
library(forecast) #forecasting methodologies
library(quantmod) #for obtaining stock prices from yahoo
library(ggplot2) # to visualize data
library(plotly)  #for making charts
library(ggfortify) # used in conjunction with ggplot
library(tseries) # time series package
library(gridExtra) # plotting multiple graphs
library(zoo) # for carrying out merging on dataframes
library(plyr) # for merging dataframe 

# Function for merging datafrmes to aid in Plotitng
MergeDF <- function(dn,df){
  require(zoo)
  ds <-as.data.frame(window(dn))
  names(ds) <- 'observed'
  ds$month <- as.yearmon(time(window(dn)))
  
  dfit<-as.data.frame(df$fitted)
  dfit$month<-as.yearmon(time(df$fitted))
  names(dfit)[1]<-'fitted'
  
  ds<-merge(ds,dfit,all.x=T)
  ds[,'forecast'] <- NA
  ds[,'lo80'] <- NA
  ds[,'hi80'] <- NA
  ds[,'lo95'] <- NA
  ds[,'hi95'] <- NA
  
  ds<-merge(ds,dfit,all.x=T) #Merge fitted values with source and training data
  
  fcastn<-as.data.frame(df)
  fcastn$Month<-as.yearmon(row.names(fcastn))
  names(fcastn)<-c('forecast','lo80','hi80','lo95','hi95','month')
  
  library(plyr) # for combining dataframe row wise
  pd<-rbind.fill(ds,fcastn)
  return(pd)
  
}
```
#Data Collection
We have imported the data from Yahoo finance. We have extracted the daily adjusted closing price value of S&P 500 from 1st January 2006 to 31st December 2016. . The ticker symbol for S&P 500 is “GSPC”. 
```{r warning = FALSE,message= FALSE}
startDate = as.Date("2006-01-01") 

endDate = as.Date("2016-12-31")

#getting data from yahoo finance using start and end date
getSymbols("^GSPC", from = startDate, to = endDate,src="yahoo")
```
The extracted data contains seven columns -  first acting as an index, the next four columns is generally referred to as OHLC i.e Open, High, Low and Close prices of the index. The sixth column comprises the volume of trade and the seventh column i.e “Adjusted Price” is of utmost importance to our paper. In our time series forecasting techniques, we will mainly be using the “Adjusted Price” column, the reason for this can be summed up as “stock split” which is explained further with an example below.

In’R’,we use the ‘quantmod’ library to extract data of any stock. The `getSymbols()` function  takes 4 parameters – the ticker, the start date ,the end date and the source. 


`Why we are using the adjusted closing price?`
        The closing price provides us the value at which the stock closes at the end of the day. Now due to some management reason, the company decides to split the stock thereby decreasing the value of the stock price. Say if one stock of company is priced for at $100, the company splits one stock into two, thereby now the price of each stock is further split and is $50. So the opening price of the stock and the closing of the stock varies the following day. Hence for our paper we have only considered the adjusted value of the index, which takes into account the “stock split “factor.

Since we are doing monthly forecasting, our extracted data is converted into monthly format.
```{r echo = FALSE,message= FALSE}
# convert data into monthly wise
GSPC_monthly <- to.monthly(GSPC)
monthly <- as.vector(GSPC_monthly$GSPC.Adjusted)

#data is split month wise from 2006 to 2016
SNP16<- ts(monthly,c(2006,1),frequency=12)
SNP16
```

#Exploratory Analysis
Our S&P 500 Stock Index data is in the form of time-series; this means that our data exists over a continuous time interval with equal spacing between every two consecutive measurements.                Once we have our time series data object ready, we plot the object to do a good exploratory analysis on it. This enables us to make inferences about important components of the time-series data, such as trend, seasonality, heteroskedasticity, and stationary. A brief on each inference:
●	Trend: a dataset has a trend when it has either a long-term increasing or decreasing graph.
●	Seasonality:a dataset has seasonality when it has patterns that repeat over known, fixed periods of time (e.g. monthly, quarterly, yearly).
●	Heteroskedasticity:a data is heteroskedastic when its variability is not constant (i.e. its variance increases or decreases as a function of the explanatory variable).
●	Stationary:a stochastic process is called stationary if the mean and variance are constant (i.e. their joint distribution does not change over time).
We start our analysis by plotting our time series to give us a basis about how to start our modeling.
```{r echo = FALSE,message= FALSE}
S <- autoplot(SNP16, 
              main = "Plot of S&P 500 Time Series(2006-2016)", 
              ts.colour = "turquoise4", size=1.25) + 
  theme(panel.background = element_rect(fill = "gray98"),
        axis.line   = element_line(colour="black"),
        axis.line.x = element_line(colour="gray"),
        axis.line.y = element_line(colour="gray")) + 
  labs(x = "Year", y = "Adjusted Price")
ggplotly(S)
```
Later we remove the year 2016 in order to compare our forecasted values with the real values.
```{r echo = FALSE}
#data is split month wise from 2006 to 2015 
SNP15<- ts(monthly,c(2006,1),c(2015,12),frequency=12)

#plot of data from  2006 to 2015
N <- autoplot(SNP15, 
              main = "Plot of S & P 500 Time Series(2006-2015)",
              ts.colour = "turquoise4", size=1) + 
  theme(panel.background = element_rect(fill = "gray98"),
        axis.ticks  = element_blank(),
        axis.line   = element_line(colour="black"),
        axis.line.x = element_line(colour="gray"),
        axis.line.y = element_line(colour="gray")) + 
  labs(x = "Year", y = "Adjusted Price")
ggplotly(N)
```
From the plot we can see that there was certain dip in the stock prices between 2008 -2010.This is time when U.S market crashed.

When we plot, we observe a trend in our time-series, we further analyze our data by decomposing. We break-down our time-series into its seasonal component, trend, and residuals/white noise using the `stl() function`.
#Decomposition of the SNP15 data 
```{r echo = FALSE}
x <- stl(SNP15,s.window= 'periodic')
x
stl <- autoplot(x, main = "Decomposition Plot of S&P 500 (2006-2015)",ts.colour = "turquoise4") +
  theme(panel.background = element_rect(fill = "gray98"),axis.line.y   = element_line(colour="gray"),axis.line.x = element_line(colour="gray"))
ggplotly(stl)
```
#Seasonal trend of our data year wise
In the year 2009 and 2013 , the trend is reversed. From the plot we can see there are no seasonal patterns, thus showing that our data is non-stationary. 
```{r echo = FALSE}
sp <- ggseasonplot(SNP15, xlab="Year", 
                   main="Seasonal Plot S&P 500 Index - Year Wise",
                   year.labels=TRUE, year.labels.left=TRUE, 
                   col=1:20, pch=19) +
  theme(panel.background = element_rect(fill = "gray98"),
        axis.line.y = element_line(colour="gray"),
        axis.line.x = element_line(colour="gray")) 
ggplotly(sp)
```
# Data Seasonality is checked - using ets() function
Data is not seasonal
```{r echo =FALSE}
s <- ets(SNP15)
seasonal1 <- !is.null(s$seasonal)
print(seasonal1)
```
We further test whether our data in seasonal by the running the below function in ‘R’.
The ‘p-value’ is 1 indicating no seasonality in our data.
```{r}
library(fma)
fits <- ets(SNP15)
fitp <- ets(SNP15,model="ANN")
deviance <- 2*c(logLik(fits) - logLik(fitp))
df <- attributes(logLik(fits))$df - attributes(logLik(fitp))$df 
1-pchisq(deviance,df)
```

# Model Estimation
Before we build our forecasting models, we need to understand the relationship of our data points at different time lags. The lag refers to the time difference between one observation and a previous observation in the dataset. The correlation between the two observations is calculated using the Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF). 
•	Autocorrelation Function (ACF): is the correlation for time series observations with a lag.
•	Partial Autocorrelation Function (PACF): is the summary of the relationship between an observation in a time series with a lag and with the relationships of intervening observations removed
```{r echo = FALSE}
# Plot ACF and PACF for SNP15 - to determine p,d and q in our ARIMA Mdoel
#Acf(SNP15,lag.max = 60)
a <- autoplot(Acf(SNP15, plot = FALSE , lag.max =60), 
              conf.int.fill = '#0000FF', 
              conf.int.value = 0.95, conf.int.type = 'ma') + 
  theme(panel.background = element_rect(fill = "gray98"),
        axis.line.y   = element_line(colour="gray"),
        axis.line.x = element_line(colour="gray")) + 
  ggtitle("ACF and PACF plots of S&P 500")

b <- autoplot(pacf(SNP15, plot = FALSE,lag.max =60), 
              conf.int.fill = '#0000FF', 
              conf.int.value = 0.95, conf.int.type = 'ma') + 
  theme(panel.background = element_rect(fill = "gray98"),
        axis.line.y   = element_line(colour="gray"),
        axis.line.x = element_line(colour="gray")) + labs(y="PACF") 

grid.arrange(a, b)
```
From the plots we observe that:
•	Both ACF and PACF shows significant values, for now we assume that an ARMA-model will be a good fit.
•	The ACF can be used to estimate the MA-part, i.e. q-value, the PACF can be used to estimate the AR-part, i.e. p-value.
•	To estimate a model-order we look at 1.) whether the ACF values die out sufficiently, 2.) whether the ACF signals over differencing and 3.) whether the ACF and PACF show any significant and easily interpretable peaks at certain lags.
•	ACF and PACF might suggest not only one model but many from which we need to choose after considering other diagnostic tools.

#Data Stationarity 
We observed from ACF and PACF that our data in not stationary .We conduct the ADF (Augmented Dickey-Fuller Test) and KPSS (Kwiatkowski–Phillips–Schmidt–Shin Test) on our data, to make our data is non-stationary.
ADF test says differences is required if p-value is > 0.05
```{r echo = FALSE}
library(tseries) # package for using ADF Test and KPSS test for checking data stationarity
```
ADF test says differences is required if p-value is > 0.05
```{r}
adf.test(SNP15)
```
Kipps test says differences is required if p-value is < 0.05
```{r}
kpss.test(SNP15)
```
#Transforming our data from Non-Stationary to Stationary
To make our data stationary, we find the difference between the consecutive values, and plot the ACF and PACF of our differenced data.

1. Function to determine the differencing order of the data - for seasonal data
```{r echo =FALSE}
nsdiffs(SNP15)
#displays the ACF,PACF and time series plot
ggtsdisplay(SNP15)
```
Observation : We determined our data doesnt require any seasonally differencing

1. Function to determine the differencing order of the data - for non-seasonal data
```{r echo =FALSE}
# for non-seasonal part in data
ndiffs(SNP15)
```
Observation: Data is not stationary, and requires differencing.

Differencing the data by order 1 and plotting the ACf ,PACF and time series plot of the differenced data.
```{r eacho =FALSE}
SNP15diff <- diff(SNP15, differences=1)
ggtsdisplay(SNP15diff)
```
Observation: 95% of ACF and PACF are within the significance levels 



We Check if the differenced data of order 1 is stationary by carrying out ADF KPSS Test
```{r echo = FALSE}
kpss.test(SNP15diff)
adf.test(SNP15diff)
```
The differenced data of order 1 is determined  to be stationary

We now know the d part of our ARIMA model. Now we will determine the p and q part .
We plot the ACF and PACF for our differenced data.
```{r echo =FALSE}
c <- autoplot(Acf(diff(SNP15), plot = FALSE , lag.max = 60), 
              conf.int.fill = '#0000FF', 
              conf.int.value = 0.95, conf.int.type = 'ma') + 
  theme(panel.background = element_rect(fill = "gray98"),
        axis.line.y   = element_line(colour="gray"),
        axis.line.x = element_line(colour="gray")) + 
  ggtitle("ACF and PACF plots of Differenced S&P500")


d <- autoplot(pacf(diff(SNP15), plot = FALSE , lag.max = 60), 
              conf.int.fill = '#0000FF', 
              conf.int.value = 0.95, conf.int.type = 'ma') + 
  theme(panel.background = element_rect(fill = "gray98"),
        axis.line.y   = element_line(colour="gray"),
        axis.line.x = element_line(colour="gray")) + labs(y="PACF") 

grid.arrange(c, d)
```
From the above Acf and pacf we observe that there are lags at higher significane levels ,so possible models are ARMA(0,0). We observe further differencing is not needed and hence in ARIMA model d = 1.

`Principle of Parsimony says choose one with the least number of parameters`

## You can also be lazy and just call auto.arima and let it do the work for you.
```{r echo =FALSE}
auto.arima(SNP15)
xyz <-auto.arima(SNP15)
```

#1. ARIMA MODEL
We have built our ARIMA model i. e p = 0 , d =1 and q = 0. We confirm if this is the best ARIMA model for the data from running the auto.arima() function. 
```{ r echo =FALSE}
fit <- Arima(SNP15, order = c(0,1,0), include.drift = TRUE)

#forecasting the next 12 months values
# plotting standardized residuals , ACF and p -value of SNP15

summary(SP500ARIMA)
```
We further check if our ‘residuals’ are white noise and are following normal distribution. 
```{ r echo =FALSE}
tsdiag(fit)
residFit <- ggplot(data=fit, aes(residuals(fit))) + 
  geom_histogram(aes(y =..density..), 
                 col="black", fill="white" , bins = 10) +
  geom_density(col=1) +
  theme(panel.background = element_rect(fill = "gray98"),
        panel.grid.minor = element_blank(),
        axis.line   = element_line(colour="gray"),
        axis.line.x = element_line(colour="gray")) +
  ggtitle("Plot of S&P 500 ARIMA Model Residuals") 

ggplotly(residFit)
```
We notice that the distribution of the residuals in not perfect, the outliers can be attributed to the 2008 financial crisis. We now move to the forecasting stage.

Plotting the forecasted values
```{r echo =FALSE}
fit <- Arima(SNP15, order = c(0,1,0), include.drift = TRUE)

SP500ARIMA <- forecast(fit, h = 12)

x <-MergeDF(SNP15,SP500ARIMA)

library(ggplot2)

r <- ggplot(data = x, aes(x = month)) + 
  geom_line(aes(y = observed, colour = "Observed"), size = 1) + 
  geom_line(aes(y = fitted, colour = "Fitted"), size = 1) + 
  geom_line(aes(y = forecast, colour = "Forecast"), size = 1) +
  scale_colour_manual("",breaks = c("Fitted","Observed","Forecast"),values = c("Black","orange","turquoise")) + 
  geom_ribbon(aes(ymin = lo95, ymax = hi95), alpha = .25) +
  geom_ribbon(aes(ymin = lo80, ymax = hi80), alpha = .25) +
  scale_x_yearmon(name = "Month / Year") +
  scale_y_continuous(breaks = seq(0,850,by=50)) +
  theme(axis.text.x = element_text(size = 10)) + 
  ggtitle("'Arima Fit to Simulated Data\n (Orange  =Forecast, Turquoise = Fitted, Black = Data, shadow=95% conf. interval)')") +
  ylab ( " Adjusted Price of S&P500")
ggplotly(r)
```


#2. Holts - Winter Forcasting Model
Holts Winter function has alpha , beta and gamma, 
* Alpha represents over all smoothing paramneter
* Beta represents trend smoothing parameter
* Gamma represents seasonal smoothing parameter

The model by default is additive,as there is no sesonality in our data we set GAMMMA =FALSE in our Holts Winter Model
```{r echo =FALSE}
HoltWintersFit <- HoltWinters(SNP15, gamma=FALSE)

HoltWintersFitForecast <- forecast.HoltWinters(HoltWintersFit, h=12)

h <-MergeDF(SNP15,HoltWintersFitForecast)

library(ggplot2)

t <- ggplot(data = h, aes(x = month)) + 
  geom_line(aes(y = observed, colour = "Observed"), size = 1) + 
  geom_line(aes(y = fitted, colour = "Fitted"), size = 1) + 
  geom_line(aes(y = forecast, colour = "Forecast"), size = 1) +
  scale_colour_manual("",breaks = c("Fitted","Observed","Forecast"),values = c("black","orange","red")) + 
  geom_ribbon(aes(ymin = lo95, ymax = hi95), alpha = .25) +
  geom_ribbon(aes(ymin = lo80, ymax = hi80), alpha = .25) +
  scale_x_yearmon(name = "Month / Year") +
  scale_y_continuous(breaks = seq(0,850,by=50)) +
  theme(axis.text.x = element_text(size = 10)) + 
  ggtitle("'Holts Winter Model to Simulated Data\n (Orange = forecast, Red =fitted, Black = data, shadow=95% conf. interval)')") +
  ylab ( " Adjusted Price of S&P500")
ggplotly(t)
```

#3. Exponential Smoothing Method:
In this methodology, more weightage is given to recent data points. The forecast is generally a flat-line.

```{ r echo =FALSE}

MergeDF <- function(dn,df){
  require(zoo)
  ds <-as.data.frame(window(dn))
  names(ds) <- 'observed'
  ds$month <- as.yearmon(time(window(dn)))
  
  dfit<-as.data.frame(df$fitted)
  dfit$month<-as.yearmon(time(df$fitted))
  names(dfit)[1]<-'fitted'
  
  ds<-merge(ds,dfit,all.x=T)
  ds[,'forecast'] <- NA
  ds[,'lo80'] <- NA
  ds[,'hi80'] <- NA
  ds[,'lo95'] <- NA
  ds[,'hi95'] <- NA
  
  ds<-merge(ds,dfit,all.x=T) #Merge fitted values with source and training data
  
  fcastn<-as.data.frame(df)
  fcastn$Month<-as.yearmon(row.names(fcastn))
  names(fcastn)<-c('forecast','lo80','hi80','lo95','hi95','month')
  
  library(plyr) # for combining dataframe row wise
  pd<-rbind.fill(ds,fcastn)
  return(pd)
  
}


startDate = as.Date("2006-01-01") 

endDate = as.Date("2016-12-31")

#getting data from yahoo finance using start and end date
getSymbols("^GSPC", from = startDate, to = endDate,src="yahoo")

# convert data into monthly wise
GSPC_monthly <- to.monthly(GSPC)
monthly <- as.vector(GSPC_monthly$GSPC.Adjusted)

#data is split month wise from 2006 to 2016
SNP16<- ts(monthly,c(2006,1),frequency=12)
SNP15<- ts(monthly,c(2006,1),c(2015,12),frequency=12)

Expo <- ets(SNP15)

Et <- forecast(Expo,h =12)

e <- MergeDF(SNP15,Et)

ets <- ggplot(data = e, aes(x = month)) + 
  geom_line(aes(y = observed, colour = "Observed"), size = 1) + 
  geom_line(aes(y = fitted, colour = "Fitted"), size = 1) + 
  geom_line(aes(y = forecast, colour = "Forecast"), size = 1) +
  scale_colour_manual("",breaks = c("Fitted","Observed","Forecast"),values = c("black","orange","blue")) + 
  geom_ribbon(aes(ymin = lo95, ymax = hi95), alpha = .25) +
  geom_ribbon(aes(ymin = lo80, ymax = hi80), alpha = .25) +
  scale_x_yearmon(name = "Month / Year") +
  scale_y_continuous(breaks = seq(0,850,by=50)) +
  theme(axis.text.x = element_text(size = 10)) + 
  ggtitle("'Exponential Smoothing Forecasting\n (Orange = forecast, Blue = fitted, Black = data, shadow=95% conf. interval)')") +
  ylab ( " Adjusted Price of S&P500")
ggplotly(ets)

```

#4. Naive Method: 
We incorporated this methodology, as it is one of the best model for financial time series object.
```{r echo =FALSE}
naiveForecast <- naive(SNP15,h = 12)

na <- MergeDF(SNP15,naiveForecast)

Naive <- ggplot(data = na, aes(x = month)) + 
  geom_line(aes(y = observed, colour = "Observed"), size = 1) + 
  geom_line(aes(y = fitted, colour = "Fitted"), size = 1) + 
  geom_line(aes(y = forecast, colour = "Forecast"), size = 1) +
  scale_colour_manual("",breaks = c("Fitted","Observed","Forecast"),values = c("black","orange","green")) + 
  geom_ribbon(aes(ymin = lo95, ymax = hi95), alpha = .25) +
  geom_ribbon(aes(ymin = lo80, ymax = hi80), alpha = .25) +
  scale_x_yearmon(name = "Month / Year") +
  scale_y_continuous(breaks = seq(0,850,by=50)) +
  theme(axis.text.x = element_text(size = 10)) + 
  ggtitle("'Naive Forecasting\n (Orange = forecast, Green = fitted, Black = data, shadow=95% conf. interval)')") +
  ylab ( "Adjusted Price of S&P500")
ggplotly(Naive)
```

# Conclusion:
After using the above stated forecasting techniques, we now generate the error values for all our methods. We consider the MAPE (Mean Absolute Percentage Error) and MAE (Mean Absolute Error) for our Error Evaluation. This is mainly because we are forecasting stock prices which will have absolute values. There is no need to consider a range of negative and positive errors as in the case of RMSE (Root Mean Square Error). RMSE is generally used when you need a wide variation of your error, as in the case of temperatures, where we get negatives and positive values. 

#1. Forecasting Error 

Below are the forecasted values of all our models:
Table of Results
```{r echo =FALSE}
startDate = as.Date("2006-01-01") 

endDate = as.Date("2016-12-31")

#getting data from yahoo finance using start and end date
getSymbols("^GSPC", from = startDate, to = endDate,src="yahoo")

# convert data into monthly wise
GSPC_monthly <- to.monthly(GSPC)
monthly <- as.vector(GSPC_monthly$GSPC.Adjusted)

#data is split month wise from 2006 to 2016
SNP16<- ts(monthly,c(2006,1),frequency=12)
SNP15<- ts(monthly,c(2006,1),c(2015,12),frequency=12)

Expo <- ets(SNP15)

Et <- forecast(Expo,h =12)

e <- MergeDF(SNP15,Et)
pr1 <- as.data.frame(SP500ARIMA)
pr2 <- as.data.frame(HoltWintersFitForecast)
pr4 <- as.data.frame(Et)
pr5 <- as.data.frame(naiveForecast)

pr1[2:5] <- NULL
pr2[2:5] <- NULL
pr4[2:5] <- NULL
pr5[2:5] <- NULL

res1 <- cbind.data.frame(pr1,pr2,pr4,pr5)

rt <- as.data.frame(window(SNP16))
names(rt) <- 'Observed'
library(zoo)
rt$Month <- as.yearmon(time(window(SNP16)))
rt <- data.frame(rt[121:132,])
rownames(rt) <- rt$Month
results<-cbind(rt,res1)
results$Month <- NULL
colnames(results)<-c('Observed Value','ARIMA','Holts Winter', 'Exponential Smoothing','Naive')
results
```
Using the “accuracy ()” function in R we now generate the errors between the original data and the forecasted data for all our forecasting methods. 
```{r echo =FALSE}
library(forecast)
v <- as.data.frame(accuracy(results$ARIMA,results$`Observed Value`))
y <- as.data.frame(accuracy(results$`Holts Winter`,results$`Observed Value`))
et <- as.data.frame(accuracy(results$`Exponential Smoothing`,results$`Observed Value`))
nai <- as.data.frame(accuracy(results$Naive,results$`Observed Value`))

Method <- c('Arima','Holts Winter',"Exponential Smoothing","Naive")
accu <- rbind.fill(v,y,et,nai)
dispaccu <- cbind(Method ,accu)
dispaccu
```
By observing the Error chart above, we see that the Holt’s Winter model has performed the best with MAPE value – “4.33” and MAE value – “0.922“.

#2. Model Error
This is mainly a backward-assessment to find the relative difference between the fitted data generated by the forecasting model and the actual data, prior to forecasting. 
```{r echo =FALSE}
ACCUA <- as.data.frame(accuracy(fit$fitted,SNP15))




Holt <- as.ts(HoltWintersFit)
ACCUH <- as.data.frame(accuracy(Holt$fitted,SNP15))

Expo <- as.ts(Expo)
ACCUE <- as.data.frame(accuracy(Expo))

ACCUN <- as.data.frame(accuracy(naiveForecast))

Method1 <- c('Arima','Holts Winter',"Exponential Smoothing",'Naive')
accu1 <- rbind.fill(ACCUA,ACCUH,ACCUE,ACCUN)
dispaccu1 <- cbind(Method1 ,accu1)
dispaccu1 <- dispaccu1[,-(8:9)]
dispaccu1
```
By observing the above chart, we notice that ARIMA model gives us the lowest MAE and MAPE error – ‘0.5676’ and ‘3.8754’ respectively.

#To conclude, Sometimes, a model which fits the data well does not necessarily perform/forecast well! This is because a perfect fit can always be obtained by using a model with appropriate number of parameters. We should always avoid over-fitting of data, as it does not give an insight to trend/pattern of the data. 

#References
1.Advantages of the mean absolute error (MAE) over the root mean square error (RMSE) in assessing average model performance - By Cort J. Willmott*, Kenji Matsuura
2.Forecasting Uncertain Hotel Room Demand - By Mihir, Mounir and Paul ,Department of Electrical and Computer Engineering , Duke Univerisity
3.Exponential Smoothing: The State of the Art - By EVERETTE S. GARDNER, Jr.
4. Forecasting: Principles & Practice - By Rob J Hyndman
5. Forecasting methods and stock market analysis - By Virginica Rusu and Cristian Rusu
